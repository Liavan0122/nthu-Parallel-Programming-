## Implementation 
### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (‚Ñì and ùëö) were calculated.
### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
### c. Describe how you chose the block sizes B_r‚Äã and B_c‚Äã and why.
### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
### e. Justify your choices and how they relate to the blocking factors and the SRAM size.  


## Profiling Results
Provide the profiling results of following metrics on the kernel of your program using NVIDIA profiling tools. NVIDIA Profiler Guide.
- occupancy
- sm efficiency
- shared memory load/store throughput
- global load/store throughput

## Experiment & Analysis
### a. System Spec  

apollo gpu  

### b. Optimization  

Any optimizations after you port the algorithm on GPU, describe them with sentences and charts. Here are some techniques you can implement:
- Coalesced memory access
- Shared memory
- Handle bank conflict
- CUDA 2D alignment
- Occupancy optimization
- Streaming
- Others
- Additional charts with explanation and studies. The more, the better.  

## Experience & conclusion
What have you learned from this homework?
Feedback (optional)

