## Implementation 
### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (â„“ and ğ‘š) were calculated.

- Forward Pass çš„é‚è¼¯éƒ½æŒ‰ç…§specè£¡çš„Algorithm & seq-flashatten.cå…§å®¹å·®ä¸å¤š:
  
1. Query-Key å…§ç© : æ¯å€‹ Thread è¨ˆç®— Q â‹… K ^Tçš„éƒ¨åˆ†å’Œï¼Œå¾ Shared Memory ä¸­è¼‰å…¥ç›¸æ‡‰çš„ Tile è³‡æ–™ï¼Œé€²è¡Œç´¯ç©è¨ˆç®—ï¼Œä¸¦å°‡çµæœå­˜æ”¾æ–¼score_tileã€‚
2. Score_tile ä¹˜ä»¥ Scaling Factorã€‚
3. æ›´æ–°row maxã€‚
4. é‡æ–°æ­£è¦åŒ–ï¼šè‹¥ K çš„ä¸‹ä¸€å€‹ Tile è¢«è™•ç†ï¼Œå‰‡éœ€è¦é‡æ–°æ­£è¦åŒ–å…ˆå‰ç´¯ç©çš„çµæœï¼Œæ ¹æ“šæ–°çš„æœ€å¤§å€¼ m å’Œèª¿æ•´ç´¯ç©çµæœã€‚
5. èˆ‡ Value çŸ©é™£çš„ç›¸ä¹˜ï¼šå°‡ Softmax æ­£è¦åŒ–å¾Œçš„çµæœèˆ‡ V çš„ Tile ç›¸ä¹˜ã€‚æ›´æ–° l ã€‚
6. å¯«å›global memoryã€‚
   

- SRAM (Shared Memory) çš„ä½¿ç”¨:
  
```
    // Shared memory åˆ†é…
    __shared__ float query_tile[BLOCK_ROWS][TILE_SIZE_K];
    __shared__ float key_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float value_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1é¿å…bank conflict
```

- Matrix Blocking çš„ä½¿ç”¨:  

å°‡çŸ©é™£æ‹†åˆ†æˆè¼ƒå°çš„Tileï¼Œæ¯å€‹ CUDA Block è² è²¬è™•ç† BLOCK_ROWS x BLOCK_COLS (32 * 32) çš„å¤§å°çš„çŸ©é™£å€å¡Šï¼Œä¸¦åœ¨æ‹†åˆ†ç‚º Tilesï¼Œæ¯å€‹ Thread è² è²¬æ›´å°çš„å€åŸŸï¼Œè™•ç†elementsæ•¸é‡å›ºå®šç‚º TILE_ROWS x TILE_COLS (4 * 4) ã€‚é”åˆ°é«˜æ•ˆåˆ©ç”¨ CUDA çš„ä¸¦è¡Œè¨ˆç®—èƒ½åŠ›ã€‚

- ä¸­é–“çµæœ â„“ å’Œ ğ‘š :
  
mï¼šæ¯rowçš„æœ€å¤§å€¼ï¼Œç”¨æ–¼ç¢ºä¿ Softmax è¨ˆç®—çš„æ•¸å€¼ç©©å®šæ€§ã€‚  

â„“ï¼šæ¯rowçš„æŒ‡æ•¸å’Œï¼Œç”¨æ–¼æœ€å¾Œçš„æ­£è¦åŒ–ã€‚

### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.    

æŠŠçŸ©é™£åˆ‡æˆå° Blockï¼Œè®“æ¯å€‹å° Block åˆ†é…çµ¦ä¸åŒçš„ GPU ä¸¦è¡Œè™•ç†ï¼Œå‡è¨­ Qã€K çš„å¤§å°æ˜¯ sequence_length * dimensionï¼ŒæŒ‰ç…§å›ºå®šçš„ BLOCK_ROWS ç¯„åœå»åˆ‡å‰² sequence_lengthï¼ŒåŒæ¨£ä¹ŸæŒ‰ç…§å›ºå®šçš„ BLOCK_COLS ç¯„åœå»åˆ‡å‰² dimensionï¼Œç”±æ–¼ ğ‘‘ âˆˆ {32, 64}ã€ğ‘ âˆˆ {128,....}
éƒ½ç‚º32çš„å€æ•¸ï¼Œè‹¥æœ‰åˆ‡å‰²å¾Œä¸å…¨çš„æƒ…æ³éƒ½çµ¦äºˆè£œ0ã€‚é€™æ¨£ä¸€ä¾†ï¼Œæ¯å€‹å°å€å¡Šå°±æ˜¯çŸ©å½¢çš„ï¼Œé€™å€‹çŸ©å½¢æœƒè¢«åˆ†é…åˆ° GPU çš„ä¸€å€‹ Block ä¸­è™•ç†ã€‚

### c. Describe how you chose the block sizes B_râ€‹ and B_câ€‹ and why.

é€šå¸¸éƒ½æ˜¯ç…§ç’°å¢ƒçš„ä¸Šé™å»è¨­ç½®ï¼Œè€Œblocksizeæ”¯æ´æœ€å¤šçš„threadså°±æ˜¯1024ï¼Œæ‰€ä»¥æœƒé¸æ“‡32*32ï¼Œå› æ­¤æŠŠBLOCK_ROWS(B_r) ã€ BLOCK_COLS(B_c)ç›´æ¥å›ºå®šç‚º32ï¼Œæ–¹ä¾¿æ’°å¯«ã€‚

### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.  

Grid çš„çš„ç¶­åº¦èˆ‡ä½œæ¥­çš„seq_lengthã€batch_sizeæœ‰é—œã€å‹•æ…‹èª¿æ•´ã€‚
```
   dim3 blockDim(BLOCK_ROWS / TILE_COLS, BLOCK_COLS / TILE_ROWS);
   dim3 gridDim(batch_size, (sequence_length + BLOCK_ROWS - 1) / BLOCK_ROWS);
```

æ¯å€‹ Block åˆ†é…ç´„ 20 KB çš„ Shared Memoryï¼Œç”¨æ–¼æš«å­˜Qã€Kã€V çš„ Tile ä»¥åŠä¸­é–“é‹ç®—çµæœã€‚


## Profiling Results
Provide the profiling results of following metrics on the kernel of your program using NVIDIA profiling tools. NVIDIA Profiler Guide.

<table>
  <tr>
    <td>
      <img src="https://imgur.com/DB0fZR0.png"  width="1000"/>
    </td>
 </tr>
</table> 

- SM Efficiency (Multiprocessor Activity)ï¼š99.00% -> ç†æƒ³
- Achieved Occupancyï¼š(9.36%) -> éå¸¸ä¸ç†æƒ³ï¼Œæˆ‘åœ¨æƒ³Achieved Occupancyé€™éº¼ä½çš„åŸå› æ˜¯ä¸æ˜¯å› ç‚ºblocksizeæ”¯æ´32 * 32 1024å€‹threadsï¼Œä½†æ˜¯æˆ‘æ¯å€‹blockå…§éƒ¨å› ç‚ºtileå¾Œæ¯å€‹threadsè™•ç†4 * 4 = 16å€‹elementsï¼Œè€Œä¸”åŒæ™‚å•Ÿç”¨256å€‹threadsè€Œå·²ï¼Œå°è‡´é‹ç®—è³‡æºç„¡æ³•å®Œå…¨ç™¼æ®ï¼Œç®—æ˜¯ä¸€ç¨®Coalesced memory accessçš„å–æ¨ã€‚
  

## Experiment & Analysis
### a. System Spec  

apollo gpu  

### b. Optimization  

Any optimizations after you port the algorithm on GPU, describe them with sentences and charts. Here are some techniques you can implement:
- Coalesced memory access âˆš
```
    float thread_results[TILE_ROWS * TILE_COLS] = {0.0f};

***

            // å°‡çµæœå¯«å…¥score_tileä¸¦ä¹˜ä»¥scale
            for (uint rr = 0; rr < TILE_ROWS; ++rr) {
                for (uint cc = 0; cc < TILE_COLS; ++cc) {
                    int row_idx = thread_row_index*TILE_ROWS + rr;
                    int col_idx = thread_col_index*TILE_COLS + cc;
                    float val = thread_results[rr*TILE_COLS+cc]*scale;
                    if (row_idx < actual_rows && col_idx < actual_cols)
                        score_tile[row_idx][col_idx] = val;
                    else
                        score_tile[row_idx][col_idx] = -INFINITY;
                }
            }
```

- Shared memory âˆš  
```
    // Shared memory åˆ†é…
    __shared__ float query_tile[BLOCK_ROWS][TILE_SIZE_K];
    __shared__ float key_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float value_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1é¿å…bank conflict
```
- Handle bank conflict âˆš
```
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1é¿å…bank conflict
```

- CUDA 2D alignment âˆš  
```
    dim3 blockDim(BLOCK_ROWS / TILE_COLS, BLOCK_COLS / TILE_ROWS);
    dim3 gridDim(batch_size, (sequence_length + BLOCK_ROWS - 1) / BLOCK_ROWS);
```


#### Optimizations Conclusion

| Optimizations methods                | å¯¦ä½œç‹€æ…‹ | å¯¦ç¾æ–¹å¼æˆ–åŸå›                                                                           | 
|---------------------------|------------|----------------------------------------------------------------------------------------------------|
| **Coalesced Memory Access** | âœ…      | ä½¿ç”¨ Tile Blockingï¼Œç¢ºä¿global memoryè¨ªå•æ˜¯é€£çºŒçš„                                               | 
| **Shared Memory**         | âœ…        | æš«å­˜ \( Q, K, V \) å’Œ `score_tile`ï¼Œæ¸›å°‘å…¨åŸŸè¨˜æ†¶é«”è¨ªå•                                            | 
| **Handle Bank Conflict**  | âœ…        | åœ¨ `score_tile` åŠ å…¥ Paddingï¼Œé¿å… Shared Memory è¡çª                                             | 
| **CUDA 2D Alignment**     | âœ…        | ä½¿ç”¨ 2D Grid å’Œ Block é…ç½®ï¼ŒThread å°æ‡‰è³‡æ–™è¡Œå’Œåˆ—                                                  | 
| **Occupancy Optimization**| âŒ        | Block å•Ÿç”¨çš„ Threads æ•¸é‡éå°‘ï¼Œæœªèƒ½å®Œå…¨åˆ©ç”¨ GPU è³‡æº                                               | 
| **Streaming**             | âŒ        | æœªä½¿ç”¨ CUDA Streams é€²è¡Œå¤šä»»å‹™ä¸¦è¡Œè™•ç†                                                             | 
| **Others**                | âŒ        |                                                                                                 | 

#### Other Charts 
åŸºæ–¼t30æ¸¬è³‡åšæ¯”è¼ƒ

# Performance Analysis Charts

| **Execution Time vs TILE_ROWS & TILE_COLS** | **Achieved Occupancy vs TILE_ROWS & TILE_COLS** | **Execution Time vs B_r & B_c** |
|---------------------------------------------|-----------------------------------------------|---------------------------------|
| ![Execution Time vs TILE_ROWS & TILE_COLS](https://imgur.com/XOtJ69u.png) | ![Achieved Occupancy vs TILE_ROWS & TILE_COLS](https://imgur.com/XW6F15U.png) | ![Execution Time vs B_r & B_c](https://imgur.com/JvwkzDM.png) |
| å° Tile èƒ½æœ‰æ•ˆåˆ©ç”¨ Coalesced Memory Accessï¼Œæ¯å€‹ Thread è² è²¬è¼ƒå°‘çš„å·¥ä½œé‡ã€‚<br> ç•¶ Tile ç‚º 4x4 æ™‚ï¼Œèƒ½å……åˆ†ç™¼æ® Shared Memory å’Œè¨ˆç®—æ ¸å¿ƒçš„æ€§èƒ½ï¼Œé”åˆ°æœ€ä½³å¹³è¡¡ã€‚<br> å¤§ Tile å°è‡´ Shared Memory çš„ä½¿ç”¨é‡åŠ‡å¢ï¼Œæ¸›å°‘äº†èƒ½åŒæ™‚é‹è¡Œçš„ Block æ•¸é‡ï¼Œé™ä½ GPU çš„ä¸¦è¡Œæ€§å’Œ Achieved Occupancyã€‚ | å¦‚å‰é¢æ‰€çŒœæƒ³çš„æ²’éŒ¯ï¼ŒAchieved Occupancy èˆ‡ Tile å¤§å°ä¹‹é–“çš„é—œä¿‚é¡¯ç¤ºäº†ä¸¦è¡Œæ€§èˆ‡ è³‡æºåˆ†é…çš„å¹³è¡¡å•é¡Œã€‚<br> ç•¶ threads è² è²¬çš„ elements æ•¸ç›®è¶Šå¤šï¼Œé–‹å•Ÿçš„ threads è¶Šå°‘ï¼ŒåŒæ¨£åœ°ï¼ŒOccupancy å°±è¶Šå·®ã€‚ | å° Block ç„¡æ³•å……åˆ†åˆ©ç”¨ Shared Memoryã€‚<br> Block Size = 16 å’Œ 32 éƒ½æ˜¯ç†æƒ³ç‹€æ…‹ï¼Œç•¶ç„¶è¶…é 64 ä¸åˆç†ã€‚ |


åŸºæ–¼è§€å¯Ÿåˆ°çš„ï¼Œæœ€å¾Œé¸æ“‡ Tile å¤§å°ï¼ˆ4x4ï¼‰å¯ä»¥å¹³è¡¡ Achieved Occupancy å’Œè¨˜æ†¶é«”è¨ªå•æ•ˆç‡ã€‚ä¸­ç­‰ Block å¤§å°ï¼ˆ32x32ï¼‰æœ€é©åˆæœ€å¤§åŒ–åˆ©ç”¨è¨­å‚™çš„æ”¯æ´ã€‚

## Experience & conclusion
### What have you learned from this homework?
æˆ‘è¦ºå¾—é€™æ¬¡çš„ä½œæ¥­å¾ˆå®¹æ˜“åœ¨è¨˜æ†¶é«”å­˜å–çš„éƒ¨åˆ†ææ··ï¼Œå°æ–¼B N dèŠ±äº†äº›æ™‚é–“ç ”ç©¶ï¼Œå°æˆ‘ä¾†èªªå¾ˆæŠ½è±¡ï¼Œå¤šè™§æœ‹å‹çš„è€å¿ƒè§£é‡‹ï¼Œæ‰åŠæ™‚ç…è»ŠéŒ¯èª¤çš„è§€å¿µï¼Œæ¯”è¼ƒç‰¹åˆ¥çš„æ˜¯ï¼ŒæŒæ¡å¦‚ä½•é€éå¯¦é©—èª¿æ•´ä¸åŒçš„é…ç½®åƒæ•¸ï¼ˆä¾‹å¦‚ Tile å¤§å°ã€Block å¤§å°ç­‰ï¼‰ï¼Œä»¥å¯¦ç¾æ€§èƒ½çš„å¹³è¡¡èˆ‡æœ€ä½³åŒ–ã€‚æˆ‘æ¸…æ¥šæˆ‘çš„æ•ˆèƒ½ä¸¦æ²’æœ‰æœ€å¤§åŒ–threadsä½¿ç”¨ç‡ï¼Œå°‡æœƒæ˜¯æˆ‘æ­¤ä½œæ¥­æœ€å¤§çš„drawsbackï¼Œå¸Œæœ›å¾€å¾Œæˆ‘èƒ½å¯«å¾—å‡ºä¾†ã€‚é€™æ¬¡ä½œæ¥­ä¹Ÿæ˜¯å—ç›Šè‰¯å¤šï¼Œæ„Ÿè¬ã€‚


