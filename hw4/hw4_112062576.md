## Implementation 
### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (â„“ and ğ‘š) were calculated.

- Forward Pass çš„é‚è¼¯éƒ½æŒ‰ç…§specè£¡çš„Algorithm & seq-flashatten.cå…§å®¹å·®ä¸å¤š:
  
1. Query-Key å…§ç© : æ¯å€‹ Thread è¨ˆç®— Q â‹… K ^Tçš„éƒ¨åˆ†å’Œï¼Œå¾ Shared Memory ä¸­è¼‰å…¥ç›¸æ‡‰çš„ Tile è³‡æ–™ï¼Œé€²è¡Œç´¯ç©è¨ˆç®—ï¼Œä¸¦å°‡çµæœå­˜æ”¾æ–¼score_tileã€‚
2. Score_tile ä¹˜ä»¥ Scaling Factorã€‚
3. æ›´æ–°row maxã€‚
4. é‡æ–°æ­£è¦åŒ–ï¼šè‹¥ K çš„ä¸‹ä¸€å€‹ Tile è¢«è™•ç†ï¼Œå‰‡éœ€è¦é‡æ–°æ­£è¦åŒ–å…ˆå‰ç´¯ç©çš„çµæœï¼Œæ ¹æ“šæ–°çš„æœ€å¤§å€¼ m å’Œèª¿æ•´ç´¯ç©çµæœã€‚
5. èˆ‡ Value çŸ©é™£çš„ç›¸ä¹˜ï¼šå°‡ Softmax æ­£è¦åŒ–å¾Œçš„çµæœèˆ‡ V çš„ Tile ç›¸ä¹˜ã€‚æ›´æ–° l ã€‚
6. å¯«å›global memoryã€‚
   

- SRAM (Shared Memory) çš„ä½¿ç”¨:
  
```
    // Shared memory åˆ†é…
    __shared__ float query_tile[BLOCK_ROWS][TILE_SIZE_K];
    __shared__ float key_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float value_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1é¿å…bank conflict
```

- Matrix Blocking çš„ä½¿ç”¨:  

å°‡çŸ©é™£æ‹†åˆ†æˆè¼ƒå°çš„Tileï¼Œæ¯å€‹ CUDA Block è² è²¬è™•ç† BLOCK_ROWS x BLOCK_COLS (32 * 32) çš„å¤§å°çš„çŸ©é™£å€å¡Šï¼Œä¸¦åœ¨æ‹†åˆ†ç‚º Tilesï¼Œæ¯å€‹ Thread è² è²¬æ›´å°çš„å€åŸŸï¼Œè™•ç†elementsæ•¸é‡å›ºå®šç‚º TILE_ROWS x TILE_COLS (4 * 4) ã€‚é”åˆ°é«˜æ•ˆåˆ©ç”¨ CUDA çš„ä¸¦è¡Œè¨ˆç®—èƒ½åŠ›ã€‚

- ä¸­é–“çµæœ â„“ å’Œ ğ‘š :
  
mï¼šæ¯rowçš„æœ€å¤§å€¼ï¼Œç”¨æ–¼ç¢ºä¿ Softmax è¨ˆç®—çš„æ•¸å€¼ç©©å®šæ€§ã€‚  

â„“ï¼šæ¯rowçš„æŒ‡æ•¸å’Œï¼Œç”¨æ–¼æœ€å¾Œçš„æ­£è¦åŒ–ã€‚

### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.    

æŠŠçŸ©é™£åˆ‡æˆå° Blockï¼Œè®“æ¯å€‹å° Block åˆ†é…çµ¦ä¸åŒçš„ GPU ä¸¦è¡Œè™•ç†ï¼Œå‡è¨­ Qã€K çš„å¤§å°æ˜¯ sequence_length * dimensionï¼ŒæŒ‰ç…§å›ºå®šçš„ BLOCK_ROWS ç¯„åœå»åˆ‡å‰² sequence_lengthï¼ŒåŒæ¨£ä¹ŸæŒ‰ç…§å›ºå®šçš„ BLOCK_COLS ç¯„åœå»åˆ‡å‰² dimensionï¼Œç”±æ–¼ ğ‘‘ âˆˆ {32, 64}ã€ğ‘ âˆˆ {128,....}
éƒ½ç‚º32çš„å€æ•¸ï¼Œè‹¥æœ‰åˆ‡å‰²å¾Œä¸å…¨çš„æƒ…æ³éƒ½çµ¦äºˆè£œ0ã€‚é€™æ¨£ä¸€ä¾†ï¼Œæ¯å€‹å°å€å¡Šå°±æ˜¯çŸ©å½¢çš„ï¼Œé€™å€‹çŸ©å½¢æœƒè¢«åˆ†é…åˆ° GPU çš„ä¸€å€‹ Block ä¸­è™•ç†ã€‚

### c. Describe how you chose the block sizes B_râ€‹ and B_câ€‹ and why.

é€šå¸¸éƒ½æ˜¯ç…§ç’°å¢ƒçš„ä¸Šé™å»è¨­ç½®ï¼Œè€Œblocksizeæ”¯æ´æœ€å¤šçš„threadså°±æ˜¯1024ï¼Œæ‰€ä»¥æœƒé¸æ“‡32*32ï¼Œå› æ­¤æŠŠBLOCK_ROWS(B_r) ã€ BLOCK_COLS(B_c)ç›´æ¥å›ºå®šç‚º32ï¼Œæ–¹ä¾¿æ’°å¯«ã€‚

### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.  

Grid çš„çš„ç¶­åº¦èˆ‡ä½œæ¥­çš„seq_lengthã€batch_sizeæœ‰é—œã€å‹•æ…‹èª¿æ•´ã€‚
```
   dim3 blockDim(BLOCK_ROWS / TILE_COLS, BLOCK_COLS / TILE_ROWS);
   dim3 gridDim(batch_size, (sequence_length + BLOCK_ROWS - 1) / BLOCK_ROWS);
```

æ¯å€‹ Block åˆ†é…ç´„ 20 KB çš„ Shared Memoryï¼Œç”¨æ–¼æš«å­˜Qã€Kã€V çš„ Tile ä»¥åŠä¸­é–“é‹ç®—çµæœã€‚


## Profiling Results
Provide the profiling results of following metrics on the kernel of your program using NVIDIA profiling tools. NVIDIA Profiler Guide.
- occupancy
- sm efficiency
- shared memory load/store throughput
- global load/store throughput

## Experiment & Analysis
### a. System Spec  

apollo gpu  

### b. Optimization  

Any optimizations after you port the algorithm on GPU, describe them with sentences and charts. Here are some techniques you can implement:
- Coalesced memory access
- Shared memory
- Handle bank conflict
- CUDA 2D alignment
- Occupancy optimization
- Streaming
- Others
- Additional charts with explanation and studies. The more, the better.  

## Experience & conclusion
What have you learned from this homework?
Feedback (optional)

