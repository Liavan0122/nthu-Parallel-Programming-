## Implementation 
### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (ℓ and 𝑚) were calculated.

- Forward Pass 的邏輯都按照spec裡的Algorithm & seq-flashatten.c內容差不多:
  
1. Query-Key 內積 : 每個 Thread 計算 Q ⋅ K ^T的部分和，從 Shared Memory 中載入相應的 Tile 資料，進行累積計算，並將結果存放於score_tile。
2. Score_tile 乘以 Scaling Factor。
3. 更新row max。
4. 重新正規化：若 K 的下一個 Tile 被處理，則需要重新正規化先前累積的結果，根據新的最大值 m 和調整累積結果。
5. 與 Value 矩陣的相乘：將 Softmax 正規化後的結果與 V 的 Tile 相乘。更新 l 。
6. 寫回global memory。
   

- SRAM (Shared Memory) 的使用:
  
```
    // Shared memory 分配
    __shared__ float query_tile[BLOCK_ROWS][TILE_SIZE_K];
    __shared__ float key_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float value_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1避免bank conflict
```

- Matrix Blocking 的使用:  

將矩陣拆分成較小的Tile，每個 CUDA Block 負責處理 BLOCK_ROWS x BLOCK_COLS (32 * 32) 的大小的矩陣區塊，並在拆分為 Tiles，每個 Thread 負責更小的區域，處理elements數量固定為 TILE_ROWS x TILE_COLS (4 * 4) 。達到高效利用 CUDA 的並行計算能力。

- 中間結果 ℓ 和 𝑚 :
  
m：每row的最大值，用於確保 Softmax 計算的數值穩定性。  

ℓ：每row的指數和，用於最後的正規化。

### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.    

把矩陣切成小 Block，讓每個小 Block 分配給不同的 GPU 並行處理，假設 Q、K 的大小是 sequence_length * dimension，按照固定的 BLOCK_ROWS 範圍去切割 sequence_length，同樣也按照固定的 BLOCK_COLS 範圍去切割 dimension，由於 𝑑 ∈ {32, 64}、𝑁 ∈ {128,....}
都為32的倍數，若有切割後不全的情況都給予補0。這樣一來，每個小區塊就是矩形的，這個矩形會被分配到 GPU 的一個 Block 中處理。

### c. Describe how you chose the block sizes B_r​ and B_c​ and why.

通常都是照環境的上限去設置，而blocksize支援最多的threads就是1024，所以會選擇32*32，因此把BLOCK_ROWS(B_r) 、 BLOCK_COLS(B_c)直接固定為32，方便撰寫。

### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.  

Grid 的的維度與作業的seq_length、batch_size有關、動態調整。
```
   dim3 blockDim(BLOCK_ROWS / TILE_COLS, BLOCK_COLS / TILE_ROWS);
   dim3 gridDim(batch_size, (sequence_length + BLOCK_ROWS - 1) / BLOCK_ROWS);
```

每個 Block 分配約 20 KB 的 Shared Memory，用於暫存Q、K、V 的 Tile 以及中間運算結果。


## Profiling Results
Provide the profiling results of following metrics on the kernel of your program using NVIDIA profiling tools. NVIDIA Profiler Guide.

<table>
  <tr>
    <td>
      <img src="https://imgur.com/DB0fZR0.png"  width="1000"/>
    </td>
 </tr>
</table> 

- SM Efficiency (Multiprocessor Activity)：99.00% -> 理想
- Achieved Occupancy：(9.36%) -> 非常不理想，我在想Achieved Occupancy這麼低的原因是不是因為blocksize支援32 * 32 1024個threads，但是我每個block內部因為tile後每個threads處理4 * 4 = 16個elements，而且同時啟用256個threads而已，導致運算資源無法完全發揮，算是一種Coalesced memory access的取捨。
  

## Experiment & Analysis
### a. System Spec  

apollo gpu  

### b. Optimization  

Any optimizations after you port the algorithm on GPU, describe them with sentences and charts. Here are some techniques you can implement:
- Coalesced memory access √
```
    float thread_results[TILE_ROWS * TILE_COLS] = {0.0f};

***

            // 將結果寫入score_tile並乘以scale
            for (uint rr = 0; rr < TILE_ROWS; ++rr) {
                for (uint cc = 0; cc < TILE_COLS; ++cc) {
                    int row_idx = thread_row_index*TILE_ROWS + rr;
                    int col_idx = thread_col_index*TILE_COLS + cc;
                    float val = thread_results[rr*TILE_COLS+cc]*scale;
                    if (row_idx < actual_rows && col_idx < actual_cols)
                        score_tile[row_idx][col_idx] = val;
                    else
                        score_tile[row_idx][col_idx] = -INFINITY;
                }
            }
```

- Shared memory √  
```
    // Shared memory 分配
    __shared__ float query_tile[BLOCK_ROWS][TILE_SIZE_K];
    __shared__ float key_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float value_tile[BLOCK_COLS][TILE_SIZE_K];
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1避免bank conflict
```
- Handle bank conflict √
```
    __shared__ float score_tile[BLOCK_ROWS][BLOCK_COLS+1]; // +1避免bank conflict
```

- CUDA 2D alignment √  
```
    dim3 blockDim(BLOCK_ROWS / TILE_COLS, BLOCK_COLS / TILE_ROWS);
    dim3 gridDim(batch_size, (sequence_length + BLOCK_ROWS - 1) / BLOCK_ROWS);
```


#### Optimizations Conclusion

| Optimizations methods                | 實作狀態 | 實現方式或原因                                                                          | 
|---------------------------|------------|----------------------------------------------------------------------------------------------------|
| **Coalesced Memory Access** | ✅      | 使用 Tile Blocking，確保global memory訪問是連續的                                               | 
| **Shared Memory**         | ✅        | 暫存 \( Q, K, V \) 和 `score_tile`，減少全域記憶體訪問                                            | 
| **Handle Bank Conflict**  | ✅        | 在 `score_tile` 加入 Padding，避免 Shared Memory 衝突                                             | 
| **CUDA 2D Alignment**     | ✅        | 使用 2D Grid 和 Block 配置，Thread 對應資料行和列                                                  | 
| **Occupancy Optimization**| ❌        | Block 啟用的 Threads 數量過少，未能完全利用 GPU 資源                                               | 
| **Streaming**             | ❌        | 未使用 CUDA Streams 進行多任務並行處理                                                             | 
| **Others**                | ❌        |                                                                                                 | 

#### Other Charts 
基於t30測資做比較

# Performance Analysis Charts

| **Execution Time vs TILE_ROWS & TILE_COLS** | **Achieved Occupancy vs TILE_ROWS & TILE_COLS** | **Execution Time vs B_r & B_c** |
|---------------------------------------------|-----------------------------------------------|---------------------------------|
| ![Execution Time vs TILE_ROWS & TILE_COLS](https://imgur.com/XOtJ69u.png) | ![Achieved Occupancy vs TILE_ROWS & TILE_COLS](https://imgur.com/XW6F15U.png) | ![Execution Time vs B_r & B_c](https://imgur.com/JvwkzDM.png) |
| 小 Tile 能有效利用 Coalesced Memory Access，每個 Thread 負責較少的工作量。<br> 當 Tile 為 4x4 時，能充分發揮 Shared Memory 和計算核心的性能，達到最佳平衡。<br> 大 Tile 導致 Shared Memory 的使用量劇增，減少了能同時運行的 Block 數量，降低 GPU 的並行性和 Achieved Occupancy。 | 如前面所猜想的沒錯，Achieved Occupancy 與 Tile 大小之間的關係顯示了並行性與 資源分配的平衡問題。<br> 當 threads 負責的 elements 數目越多，開啟的 threads 越少，同樣地，Occupancy 就越差。 | 小 Block 無法充分利用 Shared Memory。<br> Block Size = 16 和 32 都是理想狀態，當然超過 64 不合理。 |


基於觀察到的，最後選擇 Tile 大小（4x4）可以平衡 Achieved Occupancy 和記憶體訪問效率。中等 Block 大小（32x32）最適合最大化利用設備的支援。

## Experience & conclusion
### What have you learned from this homework?
我覺得這次的作業很容易在記憶體存取的部分搞混，對於B N d花了些時間研究，對我來說很抽象，多虧朋友的耐心解釋，才及時煞車錯誤的觀念，比較特別的是，掌握如何透過實驗調整不同的配置參數（例如 Tile 大小、Block 大小等），以實現性能的平衡與最佳化。我清楚我的效能並沒有最大化threads使用率，將會是我此作業最大的drawsback，希望往後我能寫得出來。這次作業也是受益良多，感謝。


