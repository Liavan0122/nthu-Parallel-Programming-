
## 1. Implementation
### a. Which algorithm do you choose in hw3-1?  
åœ¨ hw3-1 ä¸­é¸æ“‡äº† Floyd-Warshall æ¼”ç®—æ³•ä¾†è§£æ±ºå…¨ nodes æœ€çŸ­è·¯å¾‘å•é¡Œã€‚Floyd-Warshall æ¼”ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»¥æ¯å€‹ nodes ä½œç‚ºã€Œä¸­ç¹¼é»ã€ï¼Œé€æ­¥æ›´æ–°æ‰€æœ‰ nodes å° nodes ä¹‹é–“çš„æœ€çŸ­è·é›¢ï¼Œä¸¦æª¢æŸ¥ç¶“éä¸­ç¹¼é»æ˜¯å¦æœƒç¸®çŸ­è·é›¢ï¼Œæœƒå‰‡æ›´æ–°ã€‚  

### b. How do you divide your data in hw3-2, hw3-3?  
åœ¨ hw3-2 ä¸­ï¼Œè³‡æ–™è¢«åˆ†ç‚ºå¤šå€‹å¤§å°ç‚º `ğµÃ—ğµ` çš„å€å¡Šé€²è¡Œè™•ç†ï¼Œå…¶ä¸­åœ¨B = 16&32 å’Œ B = 64ç¨å¾®ä¸åŒï¼Œæ•´é«”çŸ©é™£ d_dist è¢«åŠƒåˆ†æˆå¤šå€‹ `n/B` çš„å¤§å° `ğµÃ—ğµ` çš„å€å¡Šé€²è¡Œè™•ç†ã€‚  

ä½†ç”±æ–¼ç•¶Block size = 64 x 64, Thread blocks = 32 x 32ï¼Œåœ¨åŸ·è¡Œä¸Šæœƒæœ‰é †åºä¸Šçš„æƒ…å½¢ç™¼ç”Ÿï¼Œæ­£å¸¸çš„ç‹€æ³ä¸‹æ‡‰è©²æœƒä¾åºåŸ·è¡Œè—æ©˜ç¶ ç´«ï¼Œå› ç‚ºThreadsç„¡æ³•ä¸€æ¬¡åŒ…å«æ•´å€‹Blockã€‚å¦‚åœ–
| ![image](https://github.com/user-attachments/assets/374c8a35-eb53-41b3-ae47-b04b358bc0e8)| ![image](https://github.com/user-attachments/assets/b256df75-76fa-4c4e-9a6e-79dfd6311aef)|
|:-------------------------------------------------:|:--------------------------------------------------------------:|
| [Link](https://imgur.com/Kh8FLzz)                  | [å…§éƒ¨Link](https://imgur.com/CD3TQUf)                      |

### c. Whatâ€™s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor,#blocks, #threads)  
1. **Block Factor B** : 64ã€32ã€16
2. **Threads per blocks** : 32x32 for B = 64 & 32, 16x16 for 16ï¼Œç¬¦åˆ Apollo GPU CUDA çš„ä¸Šé™ã€‚
3. **Blocks**: æœƒå…ˆå°æ¯å€‹ n åš B çš„å€æ•¸åŒ–ï¼Œå‡è¨­ n = 39989 B = 64 ï¼Œ å‰‡ n æœƒè®Šæˆ 40000ï¼Œæ–¹ä¾¿å¾ŒçºŒåšBlockçš„åˆ†å‰²ï¼Œæ‰€ä»¥éœ€è¦(40000/64)^2 = 625*625å€‹ Blocksã€‚
 
   ```n += B - ((n % B + B - 1) % B + 1);```

### d. How do you implement the communication in hw3-3?  
ç¨‹å¼ä½¿ç”¨ OpenMP å°‡å…©å€‹åŸ·è¡Œç·’åˆ†é…çµ¦å…©å€‹ GPUï¼Œåˆ†åˆ¥ç”± id = 0 å’Œ id = 1 è™•ç†ã€‚æ¯å€‹ GPU è² è²¬è™•ç†æ•´é«”è³‡æ–™çš„ä¸€éƒ¨åˆ†ï¼Œh_Distè¢«åŠƒåˆ†ç‚ºå…©éƒ¨åˆ†å¾Œåˆ†é…çµ¦å…©å€‹ GPU è™•ç†ã€‚
å…©å€‹ GPU ä¹‹é–“çš„æºé€šæ˜¯é€é CUDA çš„é»å°é» (Peer-to-Peer, P2P) memory copy å¯¦ç¾çš„ï¼Œå…·é«”ä½¿ç”¨äº† cudaMemcpyPeerAsync å‡½æ•¸ã€‚  
ç•¶GPU 0 æ˜¯å¦éœ€è¦ GPU 1 çš„è³‡æ–™æ™‚ï¼Œæœƒå…ˆåˆ¤æ–·ï¼š
```
if (id == 0 && r >= start + size_multi_gpu) {
    cudaMemcpyPeerAsync(...);
}
```

### e. Briefly describe your implementations in diagrams, figures or sentences.
1. **è³‡æ–™åˆå§‹åŒ–èˆ‡å°é½Š**ï¼š
   * è³‡æ–™64å€æ•¸åŒ–é€²è¡Œpaddingï¼Œä½¿å¾—çŸ©é™£å¤§å° n èƒ½è¢« B æ•´é™¤ã€‚
   * æœªä½¿ç”¨çš„éƒ¨åˆ†å¡«å…¥ç„¡çª®å¤§ **INF**ï¼Œå°è§’ä½ç½®è¨­æˆ **0** ã€‚
   * ä½¿ç”¨OpenMpå¡«å…¥ h_Dist çŸ©é™£ã€‚
   * å…ˆçµ±ä¸€è®€å–åˆ°å¤§è³‡æ–™ `*buffer`ï¼Œå†ä¸€æ¬¡æ€§ `*ptr` 3åç§»é‡ä½ç½®è®€å…¥ edge æ¬Šé‡ã€‚

2. **åˆ†ä¸‰éšæ®µè¨ˆç®—**ï¼š
   * Phase 1ï¼ˆpivotè¨ˆç®—ï¼‰ï¼š**å–®ä¸€** Block è² è²¬è¨ˆç®—pivotå…§çš„æœ€çŸ­è·¯å¾‘ã€‚
   * Phase 2ï¼ˆåŒè¡Œèˆ‡åŒåˆ—å€å¡Šè¨ˆç®—ï¼‰ï¼šåˆ†åˆ¥æ›´æ–°èˆ‡pivotå€å¡ŠåŒè¡Œæˆ–åŒåˆ—çš„å€å¡Š
   * Phase 3ï¼ˆå…¶é¤˜å€å¡Šè¨ˆç®—ï¼‰ï¼šè™•ç†å…¶ä»–ä¸èˆ‡ä¸­å¿ƒå€å¡Šç›´æ¥ç›¸é—œçš„å€å¡Šã€‚
  
3. **åœ–ç¤ºåŒ–å‘ˆç¾(ä»¥B = 64ç‚ºä¾‹)**
   
| ![image](https://github.com/user-attachments/assets/42a363bd-b9d8-4030-a81b-c8a9328fb3ca)| ![image](https://github.com/user-attachments/assets/183fa13b-9854-4cc8-a609-819b6d36976a)| ![image](https://github.com/user-attachments/assets/8ef3aaa8-7182-4491-a248-a4b5d84132d9)|
|:-------------------------------------------------:|:--------------------------------------------------------------:|:--------------------------------------------------------------:|
| [Phase 1](https://imgur.com/PJyH2nA)                  | [Phase 2](https://imgur.com/voluPEM)                      | [Phase 3 ](https://imgur.com/LwXpd5a)                     | 
| åªæœ‰ 1 å€‹ Blockï¼Œå› ç‚ºåªéœ€è™•ç†pivot block (r,r)       | dim3(2, round - 1) å…©çµ„åŸ·è¡Œï¼Œåˆ†åˆ¥è™•ç† åŒè¡Œï¼ˆRow Blocksï¼‰ å’Œ åŒåˆ—ï¼ˆColumn Blocksï¼‰ã€‚ ä¸”æ¯çµ„è² è²¬è™•ç† `round - 1 `å€‹Blocks ï¼Œç¸½å…±éœ€è¦ 2x(round-1) å€‹ Blocksã€‚ ç”¨(bx)ä¾†åˆ¤æ–·ç¾éšæ®µæ­£å¸¸è™•ç† row major or column major | å¾ phase2 è³‡æ–™ä¾†ç¹¼çºŒæ›´æ–°å…¶é¤˜å€å¡Šçš„æœ€çŸ­è·¯å¾‘                      |   

4. **ç›¸é—œå„ªåŒ–èˆ‡åŒæ­¥**
   
   * æ¯å€‹ Block ä½¿ç”¨ Share memory é€²è¡Œå€å¡Šå…§çš„è¨ˆç®—ã€‚
   ```
    shared_Dist[ty*B + tx] =  d_Dist[(b_i + ty) * n + (b_i + tx)];                                     //Block upper left
    shared_Dist[ty*B + (tx + B/2)] =  d_Dist[(b_i + ty) * n + (b_j + (tx + B / 2))];                   //Block upper right
    shared_Dist[(ty + B/2)*B + tx] =  d_Dist[(b_i + ty + B/2) * n + (b_i + tx)];                       //Block upper left
    shared_Dist[(ty + B/2)*B + (tx + B/2)] =  d_Dist[(b_i + ty + B/2) * n + (b_j + (tx + B / 2))];     //Block upper left
    __syncthreads();
   ```
   
   * Phase 2 ã€ Phase 3 å¸¸é »ç¹å‘¼å«çš„ Share memory ä½ç½®æ¡ç”¨ register å„²å­˜è¨ˆç®—åŠ å¿«é€Ÿåº¦ã€‚
   ```
    // Copy data from pivot, registers faster
    int val0 =  d_Dist[(b_i + ty) * n + (b_j + tx)];                
    int val1 =  d_Dist[(b_i + ty) * n + (b_j + tx + B/2)];          
    int val2 =  d_Dist[(b_i + ty + B/2) * n + (b_j + tx)];          
    int val3 =  d_Dist[(b_i + ty + B/2) * n + (b_j + tx + B/2)];
   ```
   * unroll loop å±•é–‹ã€‚
     
     `shared_Dist[i][j]=min(shared_Dist[i][j],shared_Dist[i][k]+shared_Dist[k][j])`
     
   ```
       #pragma unroll
    for (int k = 0; k < B; ++k) {
        val0 =  min(val0, shared_memory[ty*B + k]+ shared_memory[B*B + k*B + tx]);                 
        val1 =  min(val1, shared_memory[ty*B + k]+ shared_memory[B*B + k*B + (tx + B/2)]);          
        val2 =  min(val2, shared_memory[(ty + B/2)*B + k]+ shared_memory[B*B + k*B + tx]);          
        val3 =  min(val3, shared_memory[(ty + B/2)*B + k]+ shared_memory[B*B + k*B + (tx + B/2)]);   
    }
   ```
   
   * Input ã€ Output èª¿æ•´ç‚ºé©åˆå·¨å¤§è³‡æ–™å¿«é€Ÿè®€å–å½¢å¼ï¼Œé€šå¸¸æ¡ç”¨ä¸€æ¬¡æ€§å¯«å…¥å¤§å¹…æ¸›å°‘ I/O çš„æ¬¡æ•¸ï¼Œä¸”æ¡ç”¨ Openmp å¹³è¡ŒåŒ–å¯«å…¥ã€‚
   
   ```
    // å…ˆå°‡ h_Dist çš„å…§å®¹å¯«å…¥åˆ°ä¸€å€‹æš«å­˜ç·©è¡å€ä¸­ï¼Œå†ä¸€æ¬¡æ€§å°‡è©²ç·©è¡å€çš„æ‰€æœ‰å…§å®¹å¯«å…¥æ–‡ä»¶ï¼Œé€™æ¨£å¯ä»¥å¤§å¹…æ¸›å°‘ I/O çš„æ¬¡æ•¸ï¼Œæé«˜è¼¸å‡ºé€Ÿåº¦ã€‚
    int *output_buffer = (int *)malloc(n_original * n_original * sizeof(int));

    // å¡«å…… output_buffer
    #pragma omp parallel for
    for (int i = 0; i < n_original; i++) {
        for (int j = 0; j < n_original; j++) {
            int dist_value = h_Dist[i * n + j];
            if (dist_value >= INF) {
                dist_value = 1073741823;
            }
            output_buffer[i * n_original + j] = dist_value;
        }
    }
   ```

## 2. Profiling Results (hw3-2)
testcase : c21.1  
Phase1 çš„ Achieved Occupancy ç´„ 50%ï¼Œä½† SM Efficiency åƒ… 4.22%ã€‚Phase1 åªè™•ç†ä¸€å€‹ blockï¼ŒCUDA æ ¸å¿ƒä¸­çš„è³‡æºæœªè¢«å……åˆ†åˆ©ç”¨ï¼Œé€™æ˜¯å¯ä»¥æ¥å—çš„ã€‚  
Phase2 Achieved Occupancy æ¥è¿‘ 90%ï¼ŒSM Efficiency åƒ…ç´„ 36.39%ï¼Œé€™å¯èƒ½æ˜¯æˆ‘æ•´é«”æ™‚é–“ä¸å¤ å¿«çš„ä¸»å› ã€‚  
Phase3 çœ‹èµ·ä¾†å°±å¾ˆæ­£å¸¸ï¼ŒSM Efficiency é«˜é” 99%ï¼Œè³‡æºå¹¾ä¹å®Œå…¨è¢«åˆ©ç”¨ã€‚
æ‰€æœ‰é‹ç®—è³‡æºèˆ‡æ™‚é–“éƒ½é›†ä¸­åœ¨phase3ä¹Ÿæ˜¯ç†æ‰€ç•¶ç„¶çš„ï¼Œround-1*round-1æ˜¯å¾ˆå¤§çš„å€å¡Šã€‚

| ![image](https://github.com/user-attachments/assets/7bae35cb-4105-4eb3-a57a-66809b26ea2d)| ![image](https://github.com/user-attachments/assets/623e7986-bc70-4530-9dcb-9be4264cf1d8)|
|:-------------------------------------------------:|:--------------------------------------------------------------:|
|    [Link](https://imgur.com/r343uzg)          |  [Link](https://imgur.com/er7CjX8)|  

## 3. Experiment & Analysis
### a. System Spec
Apollo CPU for sequencial code
Apollo GPU for CUDA and 2 GPU.  

### b. Blocking Factor (hw3-2)

<table>
  <tr>
    <td>
      <img src="https://i.imgur.com/lUdwRUa.png" alt="Time Comparison" width="300"/>
    </td>
    <td>
      <img src="https://i.imgur.com/VIypQDp.png" alt="Shared Load Throughput" width="300"/>
    </td>
    <td>
      <img src="https://imgur.com/uN7nCXD.png" alt="Shared Store Throughput" width="300"/>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://imgur.com/DnRybiP.png" alt="Global Load Throughput" width="300"/>
    </td>
    <td>
      <img src="https://imgur.com/aMG3Rbe.png" alt="Global Store Throughput" width="300"/>
    </td>
    <td>
      <p>!ç¬¦åˆé æœŸ!</p>
    </td>
  </tr>
</table>  


### c. Optimization (hw3-2)
âˆš Coalesced memory access  =>  `shared_memory[ty * B + tx] = d_Dist[(b_y + ty) * n + (b_x + tx)];  

âˆš Shared memory => `åŒä¸Š`  

Ã— Handle bank conflict  

âˆš CUDA 2D alignment => `dim3 block(32, 32)`  

âˆš Occupancy optimization => `Achieved Occupancy æ¥è¿‘ 90%`  

âˆš Large blocking factor => `B=64`  

âˆš Reduce communication => `Phase 2 å’Œ Phase 3æ¸›å°‘äº†å°global memoryçš„é‡è¤‡è¨ªå•`  

Ã— Streaming => `æœ‰è©¦éä½†æˆæ•ˆä¸å¥½`  


### d. Weak scalability (hw3-3)  

<table>
  <tr>
    <td>
      <img src="https://imgur.com/IOmlAWq.png" alt="Weak scalability" width="500"/>
    </td>
    <td>
      <p> å•é¡Œè¦æ¨¡ï¼ˆn*nï¼‰å¾ 835,845,921ï¼ˆGPU 1ï¼‰ å¢åŠ åˆ° 1,588,580,449ï¼ˆGPU 2ï¼‰ï¼Œå¤§ç´„å¢åŠ äº† 1.90 å€ã€‚<br> å°æ‡‰çš„åŸ·è¡Œæ™‚é–“å¾ 32.59 ms å¢åŠ åˆ° 42.49 msï¼Œåƒ…å¢åŠ äº† 1.30 å€ã€‚ </p>
    </td>
  <tr>
   <td>
     <p> GPU1 : p29k1 n = 28911 n*n = 835845921</p>
   </td>
   <td>
     <p> GPU2 : c06.1 n = 39857 n*n = 1588580449</p>
   </td>
 </tr>
</table>  

### e. Time Distribution (hw3-2)
Analyze the time spent in Testcases = p29k1 B = 64 :

<table>
  <tr>
    <td>
      <img src="https://imgur.com/saFBoBn.png" alt="Time Distribution" width="500"/>
    </td>
  <tr>
   <td>
     <p>I/O Input Time (è—è‰²)ï¼š3.29 ç§’ <br>
        I/O Output Time (ç¶ è‰²)ï¼š6.16 ç§’ <br>
        Compute Time (ç´…è‰²)ï¼š21.40 ç§’ <br>
        Memory Copy (H2D, æ©™è‰²)ï¼š0.00904 ç§’ <br>
        Memory Copy (D2H, ç´«è‰²)ï¼š0.00819 ç§’</p>
   </td>
 </tr>
</table>  

### f. Others
å„ªåŒ–äº†input & output è®€å–çš„æ–¹å¼ï¼Œä¸»è¦æ¦‚å¿µå°±æ˜¯å…ˆè®€å–åˆ°bufferï¼Œåœ¨ä¸€æ¬¡æ€§å¯«å…¥ï¼Œå°æ–¼å¤§ç­†è³‡æ–™æœ‰æ•ˆï¼Œä»¥ä¸‹å…©æ®µç¨‹å¼ç¢¼æ˜¯å°šæœªå„ªåŒ–çš„æ–¹å¼ã€‚
```
I/O Input Time: 4.57 -> 3.29 seconds
I/O output Time: 25.66 -> 6.16 seconds
```

 ```
    // Initialize h_Dist with INF and 0 for diagonal elements
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            if (i < n_original && j < n_original) {
                h_Dist[i * n + j] = (i == j) ? 0 : INF;
            } else {
                h_Dist[i * n + j] = INF;
            }
        }
    }

    // Read edges
    int pair[3];
    for (int i = 0; i < m; ++i) {
        fread(pair, sizeof(int), 3, file);
        int idx = pair[0]*n + pair[1];
        h_Dist[idx] = pair[2];
    }
 ```

```
    // Write results to file
    for (int i = 0; i < n_original; i++) {
        for (int j = 0; j < n_original; j++) {
            int dist_value = h_Dist[i * n + j];


            // æª¢æŸ¥æ˜¯å¦ç„¡æ³•åˆ°é”ï¼Œè‹¥ç„¡æ³•åˆ°é”å‰‡è¼¸å‡ºç‚º 1073741823
            if (dist_value >= INF) {
                dist_value = 1073741823;
            }
            // printf("%d\t", dist_value);

            fwrite(&dist_value, sizeof(int), 1, file);
        }
        // printf("\n");
    }
```
## 4. Experiment on AMD GPU
### a. Use the method we taught in lab3 to run your GPU version code on AMD GPU.
### b. Compare the difference between Nvidia GPU and AMD GPU.
### c. Share your insight and reflection on the AMD GPU experiment.
### d. You need to run the single GPU and multi GPU version on AMD, note that when
running multi GPU on AMD node, the judge might get some error, if you encounter
this, just mention it in the report.
### e. The experiment on AMD GPU would only be part of your report score, it wonâ€™t affect
your correctness and performance score.  

## 5. Experience & conclusion
èŠ±å¾ˆå¤šæ™‚é–“é‘½ç ”è¨˜æ†¶é«”ä½ç½®ï¼Œå°¤å…¶æ˜¯share memoryçš„å„ªåŒ–ï¼Œå¾ˆå¸¸æŠ“éŒ¯ä½ç½®ï¼Œä¸èªçœŸç•«ä¸€äº›åœ–ä¹ŸæœƒéŒ¯äº‚ï¼Œç”šè‡³åœ¨åˆ†é…block size å’Œ thread block sizeä¹ŸèŠ±æ™‚é–“é‘½ç ”æ¸…æ¥šï¼ŒçœŸçš„å­¸åˆ°å¾ˆå¤šï¼Œè¦æ­»äº†ã€‚
