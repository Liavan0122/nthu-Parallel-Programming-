
## 1. Implementation
### a. Which algorithm do you choose in hw3-1?  
åœ¨ hw3-1 ä¸­é¸æ“‡äº† Floyd-Warshall æ¼”ç®—æ³•ä¾†è§£æ±ºå…¨ nodes æœ€çŸ­è·¯å¾‘å•é¡Œã€‚Floyd-Warshall æ¼”ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»¥æ¯å€‹ nodes ä½œç‚ºã€Œä¸­ç¹¼é»ã€ï¼Œé€æ­¥æ›´æ–°æ‰€æœ‰ nodes å° nodes ä¹‹é–“çš„æœ€çŸ­è·é›¢ï¼Œä¸¦æª¢æŸ¥ç¶“éä¸­ç¹¼é»æ˜¯å¦æœƒç¸®çŸ­è·é›¢ï¼Œæœƒå‰‡æ›´æ–°ã€‚  

### b. How do you divide your data in hw3-2, hw3-3?  
åœ¨ hw3-2 ä¸­ï¼Œè³‡æ–™è¢«åˆ†ç‚ºå¤šå€‹å¤§å°ç‚º `ğµÃ—ğµ` çš„å€å¡Šé€²è¡Œè™•ç†ï¼Œå…¶ä¸­åœ¨B = 16&32 å’Œ B = 64ç¨å¾®ä¸åŒï¼Œæ•´é«”çŸ©é™£ d_dist è¢«åŠƒåˆ†æˆå¤šå€‹ `n/B` çš„å¤§å° `ğµÃ—ğµ` çš„å€å¡Šé€²è¡Œè™•ç†ã€‚  
åˆ†ä¸‰å€‹éšæ®µï¼ˆphase1ã€phase2ã€phase3ï¼‰åˆ†åˆ¥æ›´æ–°æœ€çŸ­è·¯å¾‘æ¬Šé‡ï¼Œä¾ç…§specè£¡çš„æŒ‡ç¤ºï¼š
* Phase 1: æ›´æ–°ç•¶å‰å€å¡Šï¼ˆå³ä¸­å¿ƒå€å¡Šï¼‰ã€‚
* Phase 2: æ›´æ–°èˆ‡ä¸­å¿ƒå€å¡Šåœ¨åŒä¸€è¡Œæˆ–åŒä¸€åˆ—çš„å…¶ä»–å€å¡Šã€‚
* Phase 3: æ›´æ–°æ‰€æœ‰å…¶ä»–å€å¡Šï¼ˆæ—¢ä¸èˆ‡ä¸­å¿ƒå€å¡ŠåŒè¡Œä¹Ÿä¸åŒåˆ—ï¼‰ã€‚

| ![image]() | ![image]() |
|:-------------------------------------------------:|:--------------------------------------------------------------:|
| Time Taken to Calculate the Range                  | Time Taken for Mandelbrot Set Calculation                      | 

### c. Whatâ€™s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor,#blocks, #threads)  
1. **Block Factor B** : 64ã€32ã€16
2. **Threads per blocks** : 32x32 for B = 64 & 32, 16x16 for 16ï¼Œç¬¦åˆ Apollo GPU CUDA çš„ä¸Šé™ã€‚

4. **Blocks**: æœƒå…ˆå°æ¯å€‹ n åš B çš„å€æ•¸åŒ–ï¼Œå‡è¨­ n = 39989 B = 64 ï¼Œ å‰‡ n æœƒè®Šæˆ 40000ï¼Œæ–¹ä¾¿å¾ŒçºŒåšBlockçš„åˆ†å‰²ï¼Œæ‰€ä»¥éœ€è¦(40000/64)^2 = 625*625å€‹ Blocks ã€‚


### d. How do you implement the communication in hw3-3?
### e. Briefly describe your implementations in diagrams, figures or sentences.

## 2. Profiling Results (hw3-2)
Provide the profiling results of following metrics on the biggest kernel of your program
using NVIDIA profiling tools. NVIDIA Profiler Guide.
### â—‹ occupancy
### â—‹ sm efficiency
### â—‹ shared memory load/store throughput
### â—‹ global load/store throughput

## 3. Experiment & Analysis
### a. System Spec
If you didnâ€™t use our Apollo server for the experiments, please show the CPU, RAM,
disk of the system.
### b. Blocking Factor (hw3-2)
Observe what happened with different blocking factors, and plot the trend in
terms of Integer GOPS and global/shared memory bandwidth. (You can get the
information from profiling tools or manual) (You might want to check nvprof and
Metrics Reference)

### c. Optimization (hw3-2)
Any optimizations after you port the algorithm on GPU, describe them with
sentences and charts. Here are some techniques you can implement:
â–  Coalesced memory access
â–  Shared memory
â–  Handle bank conflict
â–  CUDA 2D alignment
â–  Occupancy optimization
â–  Large blocking factor
â–  Reduce communication
â–  Streaming

### d. Weak scalability (hw3-3)
Observe weak scalability of the multi-GPU implementations
e. Time Distribution (hw3-2)
Analyze the time spent in:
â— computing
â— communication
â— memory copy (H2D, D2H)
â— I/O of your program w.r.t. input size.
### f. Others
Additional charts with explanation and studies. The more, the better.  

## 4. Experiment on AMD GPU
### a. Use the method we taught in lab3 to run your GPU version code on AMD GPU.
### b. Compare the difference between Nvidia GPU and AMD GPU.
### c. Share your insight and reflection on the AMD GPU experiment.
### d. You need to run the single GPU and multi GPU version on AMD, note that when
running multi GPU on AMD node, the judge might get some error, if you encounter
this, just mention it in the report.
### e. The experiment on AMD GPU would only be part of your report score, it wonâ€™t affect
your correctness and performance score.  

## 5. Experience & conclusion
