
## 1. Implementation
### a. Which algorithm do you choose in hw3-1?  
在 hw3-1 中選擇了 Floyd-Warshall 演算法來解決全 nodes 最短路徑問題。Floyd-Warshall 演算法的核心思想是以每個 nodes 作為「中繼點」，逐步更新所有 nodes 對 nodes 之間的最短距離，並檢查經過中繼點是否會縮短距離，會則更新。  

### b. How do you divide your data in hw3-2, hw3-3?  
在 hw3-2 中，資料被分為多個大小為 `𝐵×𝐵` 的區塊進行處理，其中在B = 16&32 和 B = 64稍微不同，整體矩陣 d_dist 被劃分成多個 `n/B` 的大小 `𝐵×𝐵` 的區塊進行處理。  

但由於當Block size = 64 x 64, Thread blocks = 32 x 32，在執行上會有順序上的情形發生，正常的狀況下應該會依序執行藍橘綠紫，因為Threads無法一次包含整個Block。如圖
| ![image](https://github.com/user-attachments/assets/374c8a35-eb53-41b3-ae47-b04b358bc0e8)| ![image](https://github.com/user-attachments/assets/b256df75-76fa-4c4e-9a6e-79dfd6311aef)|
|:-------------------------------------------------:|:--------------------------------------------------------------:|
| [Link](https://imgur.com/Kh8FLzz)                  | [內部Link](https://imgur.com/CD3TQUf)                      |

### c. What’s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor,#blocks, #threads)  
1. **Block Factor B** : 64、32、16
2. **Threads per blocks** : 32x32 for B = 64 & 32, 16x16 for 16，符合 Apollo GPU CUDA 的上限。
3. **Blocks**: 會先對每個 n 做 B 的倍數化，假設 n = 39989 B = 64 ， 則 n 會變成 40000，方便後續做Block的分割，所以需要(40000/64)^2 = 625*625個 Blocks。
 
   ```n += B - ((n % B + B - 1) % B + 1);```

### d. How do you implement the communication in hw3-3?  
程式使用 OpenMP 將兩個執行緒分配給兩個 GPU，分別由 id = 0 和 id = 1 處理。每個 GPU 負責處理整體資料的一部分，h_Dist被劃分為兩部分後分配給兩個 GPU 處理。
兩個 GPU 之間的溝通是透過 CUDA 的點對點 (Peer-to-Peer, P2P) memory copy 實現的，具體使用了 cudaMemcpyPeerAsync 函數。  
當GPU 0 是否需要 GPU 1 的資料時，會先判斷：
```
if (id == 0 && r >= start + size_multi_gpu) {
    cudaMemcpyPeerAsync(...);
}
```

### e. Briefly describe your implementations in diagrams, figures or sentences.
1. **資料初始化與對齊**：
   * 資料64倍數化進行padding，使得矩陣大小 n 能被 B 整除。
   * 未使用的部分填入無窮大 **INF**，對角位置設成 **0** 。
   * 使用OpenMp填入 h_Dist 矩陣。
   * 先統一讀取到大資料 `*buffer`，再一次性 `*ptr` 3偏移量位置讀入 edge 權重。

2. **分三階段計算**：
   * Phase 1（pivot計算）：**單一** Block 負責計算pivot內的最短路徑。
   * Phase 2（同行與同列區塊計算）：分別更新與pivot區塊同行或同列的區塊
   * Phase 3（其餘區塊計算）：處理其他不與中心區塊直接相關的區塊。
  
3. **圖示化呈現(以B = 64為例)**
   
| ![image](https://github.com/user-attachments/assets/42a363bd-b9d8-4030-a81b-c8a9328fb3ca)| ![image](https://github.com/user-attachments/assets/183fa13b-9854-4cc8-a609-819b6d36976a)| ![image](https://github.com/user-attachments/assets/8ef3aaa8-7182-4491-a248-a4b5d84132d9)|
|:-------------------------------------------------:|:--------------------------------------------------------------:|:--------------------------------------------------------------:|
| [Phase 1](https://imgur.com/PJyH2nA)                  | [Phase 2](https://imgur.com/voluPEM)                      | [Phase 3 ](https://imgur.com/LwXpd5a)                     | 
| 只有 1 個 Block，因為只需處理pivot block (r,r)       | dim3(2, round - 1) 兩組執行，分別處理 同行（Row Blocks） 和 同列（Column Blocks）。 且每組負責處理 `round - 1 `個Blocks ，總共需要 2x(round-1) 個 Blocks。 用(bx)來判斷現階段正常處理 row major or column major | 從 phase2 資料來繼續更新其餘區塊的最短路徑                      |   

4. **相關優化與同步**
   
   * 每個 Block 使用 Share memory 進行區塊內的計算。
   ```
    shared_Dist[ty*B + tx] =  d_Dist[(b_i + ty) * n + (b_i + tx)];                                     //Block upper left
    shared_Dist[ty*B + (tx + B/2)] =  d_Dist[(b_i + ty) * n + (b_j + (tx + B / 2))];                   //Block upper right
    shared_Dist[(ty + B/2)*B + tx] =  d_Dist[(b_i + ty + B/2) * n + (b_i + tx)];                       //Block upper left
    shared_Dist[(ty + B/2)*B + (tx + B/2)] =  d_Dist[(b_i + ty + B/2) * n + (b_j + (tx + B / 2))];     //Block upper left
    __syncthreads();
   ```
   
   * Phase 2 、 Phase 3 常頻繁呼叫的 Share memory 位置採用 register 儲存計算加快速度。
   ```
    // Copy data from pivot, registers faster
    int val0 =  d_Dist[(b_i + ty) * n + (b_j + tx)];                
    int val1 =  d_Dist[(b_i + ty) * n + (b_j + tx + B/2)];          
    int val2 =  d_Dist[(b_i + ty + B/2) * n + (b_j + tx)];          
    int val3 =  d_Dist[(b_i + ty + B/2) * n + (b_j + tx + B/2)];
   ```
   * unroll loop 展開。
     
     `shared_Dist[i][j]=min(shared_Dist[i][j],shared_Dist[i][k]+shared_Dist[k][j])`
     
   ```
       #pragma unroll
    for (int k = 0; k < B; ++k) {
        val0 =  min(val0, shared_memory[ty*B + k]+ shared_memory[B*B + k*B + tx]);                 
        val1 =  min(val1, shared_memory[ty*B + k]+ shared_memory[B*B + k*B + (tx + B/2)]);          
        val2 =  min(val2, shared_memory[(ty + B/2)*B + k]+ shared_memory[B*B + k*B + tx]);          
        val3 =  min(val3, shared_memory[(ty + B/2)*B + k]+ shared_memory[B*B + k*B + (tx + B/2)]);   
    }
   ```
   
   * Input 、 Output 調整為適合巨大資料快速讀取形式，通常採用一次性寫入大幅減少 I/O 的次數，且採用 Openmp 平行化寫入。
   
   ```
    // 先將 h_Dist 的內容寫入到一個暫存緩衝區中，再一次性將該緩衝區的所有內容寫入文件，這樣可以大幅減少 I/O 的次數，提高輸出速度。
    int *output_buffer = (int *)malloc(n_original * n_original * sizeof(int));

    // 填充 output_buffer
    #pragma omp parallel for
    for (int i = 0; i < n_original; i++) {
        for (int j = 0; j < n_original; j++) {
            int dist_value = h_Dist[i * n + j];
            if (dist_value >= INF) {
                dist_value = 1073741823;
            }
            output_buffer[i * n_original + j] = dist_value;
        }
    }
   ```

## 2. Profiling Results (hw3-2)
testcase : c21.1  
Phase1 的 Achieved Occupancy 約 50%，但 SM Efficiency 僅 4.22%。Phase1 只處理一個 block，CUDA 核心中的資源未被充分利用，這是可以接受的。  
Phase2 Achieved Occupancy 接近 90%，SM Efficiency 僅約 36.39%，這可能是我整體時間不夠快的主因。  
Phase3 看起來就很正常，SM Efficiency 高達 99%，資源幾乎完全被利用。
所有運算資源與時間都集中在phase3也是理所當然的，round-1*round-1是很大的區塊。

| ![image](https://github.com/user-attachments/assets/7bae35cb-4105-4eb3-a57a-66809b26ea2d)| ![image](https://github.com/user-attachments/assets/623e7986-bc70-4530-9dcb-9be4264cf1d8)|
|:-------------------------------------------------:|:--------------------------------------------------------------:|
|    [Link](https://imgur.com/r343uzg)          |  [Link](https://imgur.com/er7CjX8)|  

## 3. Experiment & Analysis
### a. System Spec
Apollo CPU for sequencial code
Apollo GPU for CUDA and 2 GPU.  

### b. Blocking Factor (hw3-2)

<table>
  <tr>
    <td>
      <img src="https://i.imgur.com/lUdwRUa.png" alt="Time Comparison" width="300"/>
    </td>
    <td>
      <img src="https://i.imgur.com/VIypQDp.png" alt="Shared Load Throughput" width="300"/>
    </td>
    <td>
      <img src="https://imgur.com/uN7nCXD.png" alt="Shared Store Throughput" width="300"/>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://imgur.com/DnRybiP.png" alt="Global Load Throughput" width="300"/>
    </td>
    <td>
      <img src="https://imgur.com/aMG3Rbe.png" alt="Global Store Throughput" width="300"/>
    </td>
    <td>
      <p>!符合預期!</p>
    </td>
  </tr>
</table>  


### c. Optimization (hw3-2)
√ Coalesced memory access  =>  `shared_memory[ty * B + tx] = d_Dist[(b_y + ty) * n + (b_x + tx)];  

√ Shared memory => `同上`  

× Handle bank conflict  

√ CUDA 2D alignment => `dim3 block(32, 32)`  

√ Occupancy optimization => `Achieved Occupancy 接近 90%`  

√ Large blocking factor => `B=64`  

√ Reduce communication => `Phase 2 和 Phase 3減少了對global memory的重複訪問`  

× Streaming => `有試過但成效不好`  


### d. Weak scalability (hw3-3)  

<table>
  <tr>
    <td>
      <img src="https://imgur.com/IOmlAWq.png" alt="Weak scalability" width="500"/>
    </td>
    <td>
      <p> 問題規模（n*n）從 835,845,921（GPU 1） 增加到 1,588,580,449（GPU 2），大約增加了 1.90 倍。<br> 對應的執行時間從 32.59 ms 增加到 42.49 ms，僅增加了 1.30 倍。 </p>
    </td>
  <tr>
   <td>
     <p> GPU1 : p29k1 n = 28911 n*n = 835845921</p>
   </td>
   <td>
     <p> GPU2 : c06.1 n = 39857 n*n = 1588580449</p>
   </td>
 </tr>
</table>  

### e. Time Distribution (hw3-2)
Analyze the time spent in Testcases = p29k1 B = 64 :

<table>
  <tr>
    <td>
      <img src="https://imgur.com/saFBoBn.png" alt="Time Distribution" width="500"/>
    </td>
  <tr>
   <td>
     <p>I/O Input Time (藍色)：3.29 秒 <br>
        I/O Output Time (綠色)：6.16 秒 <br>
        Compute Time (紅色)：21.40 秒 <br>
        Memory Copy (H2D, 橙色)：0.00904 秒 <br>
        Memory Copy (D2H, 紫色)：0.00819 秒</p>
   </td>
 </tr>
</table>  

### f. Others
優化了input & output 讀取的方式，主要概念就是先讀取到buffer，在一次性寫入，對於大筆資料有效，以下兩段程式碼是尚未優化的方式。
```
I/O Input Time: 4.57 -> 3.29 seconds
I/O output Time: 25.66 -> 6.16 seconds
```

 ```
    // Initialize h_Dist with INF and 0 for diagonal elements
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            if (i < n_original && j < n_original) {
                h_Dist[i * n + j] = (i == j) ? 0 : INF;
            } else {
                h_Dist[i * n + j] = INF;
            }
        }
    }

    // Read edges
    int pair[3];
    for (int i = 0; i < m; ++i) {
        fread(pair, sizeof(int), 3, file);
        int idx = pair[0]*n + pair[1];
        h_Dist[idx] = pair[2];
    }
 ```

```
    // Write results to file
    for (int i = 0; i < n_original; i++) {
        for (int j = 0; j < n_original; j++) {
            int dist_value = h_Dist[i * n + j];


            // 檢查是否無法到達，若無法到達則輸出為 1073741823
            if (dist_value >= INF) {
                dist_value = 1073741823;
            }
            // printf("%d\t", dist_value);

            fwrite(&dist_value, sizeof(int), 1, file);
        }
        // printf("\n");
    }
```
## 4. Experiment on AMD GPU
### a. Use the method we taught in lab3 to run your GPU version code on AMD GPU.
### b. Compare the difference between Nvidia GPU and AMD GPU.
### c. Share your insight and reflection on the AMD GPU experiment.
### d. You need to run the single GPU and multi GPU version on AMD, note that when
running multi GPU on AMD node, the judge might get some error, if you encounter
this, just mention it in the report.
### e. The experiment on AMD GPU would only be part of your report score, it won’t affect
your correctness and performance score.  

## 5. Experience & conclusion
花很多時間鑽研記憶體位置，尤其是share memory的優化，很常抓錯位置，不認真畫一些圖也會錯亂，甚至在分配block size 和 thread block size也花時間鑽研清楚，真的學到很多，要死了。
