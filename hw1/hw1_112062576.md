## Title, name, student ID
楊峻銘 112062576


## Implementation
* Load Balance
  平均分配相同大小的data給每個processors，若分配後還有k個，則把前k個processor +1個data，同時offset代表每個processor開始的位置。
  ```
    int each_rank_data = n / size;
    int remain = n % size;
    MPI_Offset offset = (each_rank_data * rank + std::min(remain, rank)) * sizeof(float);
    int each_rank_data_length = each_rank_data + (rank < remain);
  ```
* MPI In
  由於每個processor都已知開始讀取的位置，彼此區間不影響，所以採用`MPI_File_read_at`，可以同時平行讀取，加快效能。
  ```
    MPI_File input_file;
    MPI_File_open(MPI_COMM_WORLD, input_filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &input_file);
    MPI_File_read_at(input_file, offset, data, each_rank_data_length, MPI_FLOAT, MPI_STATUS_IGNORE);
    MPI_File_close(&input_file);
  ```
* Local Sort
  由於教授上課說過，各區間內的處理可以自由發揮，所以我先讓各區間的資料內部排序，方便後續作sorting的速度，在此有嘗試兩種sorting，以結果來說spreadsort較快

  
  `boost::sort::spreadsort::spreadsort`

  `Radix Sort`
  
* Communication Neighbor
  傳遞和接收資料的rank鄰居，分別用`rank_neighbor`紀錄，以圖所示，以rank為奇數情況來說，`rank_neighbor[0]`存放even phase交互的對象，`rank_neighbor[1]`存放odd phase交互的對象，而rank偶數的情況下則反之，同時也要考慮第一個與最後一個rank邊界交互問題。
  
| ![image](https://github.com/user-attachments/assets/9705338c-e9a0-4475-904c-61fc37ad6b41) |
|:-------------------------------------------------:|
| [Imgur](https://imgur.com/8riSbcO)       |  

  ```
    int rank_neighbor[2], rank_neighbor_length[2];
    if(rank & 1){
        rank_neighbor[0] = rank - 1;
        rank_neighbor_length[0] = (rank_neighbor[0] < 0) ? 0 : each_rank_data + (rank_neighbor[0] < remain);
        rank_neighbor[1] = (rank + 1 == size) ? MPI_PROC_NULL : rank + 1;
        rank_neighbor_length[1] = (rank_neighbor[1] == MPI_PROC_NULL) ? 0 : each_rank_data + (rank_neighbor[1] < remain);
    }else{
        rank_neighbor[0] = (rank + 1 == size) ? MPI_PROC_NULL : rank + 1;
        rank_neighbor_length[0] = (rank_neighbor[0] == MPI_PROC_NULL) ? 0 : each_rank_data + (rank_neighbor[0] < remain);
        rank_neighbor[1] = (rank == 0) ? MPI_PROC_NULL : rank - 1;
        rank_neighbor_length[1] = (rank_neighbor[1] < 0) ? 0 : each_rank_data + (rank_neighbor[1] < remain);
    }
  ```
  
* Odd-Even Sort
  每個for迴圈開始時，先取得鄰居資訊，並藉由 `MPI_Isend` 和 `MPI_Irecv` 傳遞資料給neighbors，進行**mergeData function**，以mergeFromMin(neighbor_rank < current_rank)判定tempBuffer vector存取資料對象，
  前幾次發現做完都會有排序不完全的問題，後來發現原來是要做process size+1 次數，因為交換過程的worst case，會從第一個process 的 data 交換到最後個process也說不定，所以需要size+1 次 for。
  
舉例來說    
```
1, 5 先做對比，小的放入buffer，再來4, 5作對比，依此類推，最後 buffer swap current rank, 且把剩餘的放入neighbor rank中。 
```
  

| ![image](https://github.com/user-attachments/assets/3906a002-8ca0-4944-be8d-215bd650567d) |
|:-------------------------------------------------:|
| [Imgur](https://imgur.com/KS3HQzy)  |
  

  ```
    for (int i = 0; i < size + 1; i++) {
        bool isOddPhase = i % 2;

        int neighbor;
        int neighbor_length;

        if (isOddPhase) {
            neighbor = rank_neighbor[1];
            neighbor_length = rank_neighbor_length[1];
        } else {
            neighbor = rank_neighbor[0];
            neighbor_length = rank_neighbor_length[0];
        }

        if (neighbor == MPI_PROC_NULL) {
            continue;
        }

        MPI_Request send_request, recv_request;

        MPI_Isend(data, each_rank_data_length, MPI_FLOAT, neighbor, 0, MPI_COMM_WORLD, &send_request);

        MPI_Irecv(recvData, neighbor_length, MPI_FLOAT, neighbor, 0, MPI_COMM_WORLD, &recv_request);

        MPI_Wait(&send_request, MPI_STATUS_IGNORE);
        MPI_Wait(&recv_request, MPI_STATUS_IGNORE);

        if (neighbor_length > 0) {
            bool mergeFromMin = (rank < neighbor);
            mergeData(data, each_rank_data_length, recvData, neighbor_length, tempBuffer, mergeFromMin);
            std::swap(data, tempBuffer);
        }
    }
  ```
  
  ```
  void mergeData(float* localData, int localSize, float* neighborData, int neighborSize, float* tempBuffer, bool mergeFromMin) {
      if (mergeFromMin) {
          int localData_index = 0, neighborData_index = 0;
  
          for(int tempBuffer_index = 0; tempBuffer_index < localSize ; tempBuffer_index++){
              if(localData_index < localSize && (neighborData_index >= neighborSize || localData[localData_index] < neighborData[neighborData_index])){
                  tempBuffer[tempBuffer_index] = localData[localData_index++];
              }else{
                  tempBuffer[tempBuffer_index] = neighborData[neighborData_index++];
              }
          }
      } else {
          int localData_index = localSize - 1;
          int neighborData_index = neighborSize - 1;
          
          for(int tempBuffer_index = localSize - 1; tempBuffer_index >= 0 ; tempBuffer_index--){
              if(localData_index >= 0 && (neighborData_index < 0 || localData[localData_index] > neighborData[neighborData_index])){
                  tempBuffer[tempBuffer_index] = localData[localData_index--];
              }else{
                  tempBuffer[tempBuffer_index] = neighborData[neighborData_index--];
              }
          }
      }
  }
  ```


* MPI Out
  同理MPI_In。
  ```
    MPI_File output_file;
    MPI_File_open(MPI_COMM_WORLD, output_filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, MPI_INFO_NULL, &output_file);
    MPI_File_write_at(output_file, offset, data, each_rank_data_length, MPI_FLOAT, MPI_STATUS_IGNORE);
    MPI_File_close(&output_file);
  ```


## Experiment & Analysis  

### Methodology  

#### System Spec  

課程所提供的Apollo  

#### Performance Metrics  

I/O : IO time : MPI_READ & MPI_Write Time (藍色)  

Comm : Communication Time : MPI Commucation operation time (紅色)  

Cpu : Other cpu computation Time : 主要指每個進程在自身數據上進行計算的時間，通常都花在**spreadsort**時間上。 (綠色)  

### Plots: Speedup Factor & Profile  

#### Experimental Method:  

Test Case Description:  40.txt n: 536869888。  

Parallel Configurations: two case:  

**different processes under same 3 nodes**, and the other is **different nodes under same 12 processes number**.  

#### Performance Measurement:  

Nsight Systems 2024.6.1 & python plot  

| ![image](https://github.com/user-attachments/assets/1c16951c-78d8-4308-a59e-b0707223d78a) | ![image](https://github.com/user-attachments/assets/8e95b3c2-22fc-454b-bf75-eb5192999e63)| 
|:-------------------------------------------------:|:--------------------------------------------------------------:|
| Nsight systems [Imgur](https://imgur.com/Pr7FRBK)                 | [Imgur](https://imgur.com/qizoegI)                     | 


| ![image](https://github.com/user-attachments/assets/963e39a0-2aa9-474c-a40a-a0dd66546eaa) | ![image](https://github.com/user-attachments/assets/e5165d3e-7685-4bff-bcbc-7b6e43677374)| 
|:-------------------------------------------------:|:--------------------------------------------------------------:|
| Case 1 [Imgur](https://imgur.com/cniBUiA)                  | [Imgur](https://imgur.com/eJdlEpD)                      | 


| ![image](https://github.com/user-attachments/assets/d7ad3785-4b01-4202-923f-e570b0fc6dda) | ![image](https://github.com/user-attachments/assets/696cbbb2-b243-4a68-9961-923004367a4d)| 
|:-------------------------------------------------:|:--------------------------------------------------------------:|
| Case 2 [Imgur](https://imgur.com/dvJk4pr)                 | [Imgur](https://imgur.com/DgkGBY0)                      | 

#### Analysis of Results:
由於load balance，所以只觀測 rank 0 的時間。  

Nsight systems :  `MPI_File_close` 占用了大部分的總時間（51.3%），我推測原因應該是 MPI 文件系統的延遲或是數據在文件關閉時需要同步寫入，導致rank0一直在等待其他rank都寫入後才會關閉，那這等待時間就等於浪費了，再來 `MPI_Wait` 占用了 (25.5%) 的時間且調用了 14 次，說明在通訊操作中rank 0 需要等待其他進程完成某些操作，好像更能支持剛剛的論點。其餘看起來都蠻正常的。

Case 1 : 可以觀察到整體的時間並沒有差異太大，但 I/O 時間隨著nodes數量的增加逐漸些幅減少，意味著 I/O 開銷因為多節點而得到了分散，從而提升了 I/O 效率，但不大。Commucation 在 nodes 數增加到 4 個前明顯減少，但從 6 個 nodes 開始，時間逐漸增加，這表明當節點數量較大時，process之間的 commmucation cost time 變得更加顯著。這應該是我的的 MPI commucation bottleneck situation。而 CPU 計算時間變化不大，基本保持在 1.4 秒左右，這是因為每個進程處理的計算量並沒有隨節點增加而明顯改變。計算的負擔已經均勻分配，因此無法通過增加節點進一步加速。  

Case 2 : 這個的時間變化就很明顯了，尤其是在Other Cpu compute time的部分，更多的 processes 分擔了計算任務，因此每個 process 處理的數據量減少。當進程數量達到 24 和 36 時，計算時間接近最低，其餘的兩項倒是沒影響太大。

  
## Optimization Strategies && Disscusion:
1. 以 process 為單位在Load balance的部分，可以在process越多的情況下，平均加快CPU效能。
2. 選擇快速的sort演算法，在local排序部分加快，嘗試過bucket sort但成效不如預期，差不多但慢一些spreadsort。
3. Memory allocation事先都規劃好，不能說是好事，並竟不是動態調整，但可以統一縮限控制記憶體的部分。
4. 事先用簡易的空間記住訪問的鄰居。
5. Merge過程中使用swap，減少copy所消耗的額外時間，但在這部分應該可以做得更好，譬如說先判斷，如果左邊得大值小於右邊的最小，就可以不用 merge 直接 return之類的細節。
6. 整體的小缺點bottleneck，在節點數量過多時，通訊開銷逐漸變得主導，並且每個節點負責的計算量減少，process 間的同步和數據交換變得更頻繁，導致加速效果下降。
7. 最大的bottleneck應該是文件上的操作，尤其是文件關閉和寫入操作同步，應該可以著重在這部分優化。

## Conclusion
做完作業後，非常熟悉MPI整個通訊的使用方式做法，還有結合lab概念如何好好妥善分配每個process的loading，但我陷入sequencial平行後不太知道怎麼更有效優化的感覺，有一種盡了人事的感覺，但看到scoreboard上的同學們都很強，有點無頭緒，不管如此，這次作業獲益匪淺，讓我更了解mpi程式和nsys系統檢測。

