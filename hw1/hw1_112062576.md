## Title, name, student ID
楊峻銘 112062576


## Implementation
* Load Balance
  平均分配相同大小的data給每個processors，若分配後還有k個，則把前k個processor +1個data，同時offset代表每個processor開始的位置。
  ```
    int each_rank_data = n / size;
    int remain = n % size;
    MPI_Offset offset = (each_rank_data * rank + std::min(remain, rank)) * sizeof(float);
    int each_rank_data_length = each_rank_data + (rank < remain);
  ```
* MPI In
  由於每個processor都已知開始讀取的位置，彼此區間不影響，所以採用`MPI_File_read_at`，可以同時平行讀取，加快效能。
  ```
    MPI_File input_file;
    MPI_File_open(MPI_COMM_WORLD, input_filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &input_file);
    MPI_File_read_at(input_file, offset, data, each_rank_data_length, MPI_FLOAT, MPI_STATUS_IGNORE);
    MPI_File_close(&input_file);
  ```
* Local Sort
  由於教授上課說過，各區間內的處理可以自由發揮，所以我先讓各區間的資料內部排序，方便後續作sorting的速度，在此有嘗試兩種sorting，以結果來說spreadsort較快

  
  `boost::sort::spreadsort::spreadsort`

  `Radix Sort`
  
* Communication Neighbor
  傳遞和接收資料的rank鄰居，分別用`rank_neighbor`紀錄，以圖所示，以rank為奇數情況來說，`rank_neighbor[0]`存放even phase交互的對象，`rank_neighbor[1]`存放odd phase交互的對象，而rank偶數的情況下則反之，同時也要考慮第一個與最後一個rank邊界交互問題。
  
  ![image](https://github.com/user-attachments/assets/9705338c-e9a0-4475-904c-61fc37ad6b41)

  ```
    int rank_neighbor[2], rank_neighbor_length[2];
    if(rank & 1){
        rank_neighbor[0] = rank - 1;
        rank_neighbor_length[0] = (rank_neighbor[0] < 0) ? 0 : each_rank_data + (rank_neighbor[0] < remain);
        rank_neighbor[1] = (rank + 1 == size) ? MPI_PROC_NULL : rank + 1;
        rank_neighbor_length[1] = (rank_neighbor[1] == MPI_PROC_NULL) ? 0 : each_rank_data + (rank_neighbor[1] < remain);
    }else{
        rank_neighbor[0] = (rank + 1 == size) ? MPI_PROC_NULL : rank + 1;
        rank_neighbor_length[0] = (rank_neighbor[0] == MPI_PROC_NULL) ? 0 : each_rank_data + (rank_neighbor[0] < remain);
        rank_neighbor[1] = (rank == 0) ? MPI_PROC_NULL : rank - 1;
        rank_neighbor_length[1] = (rank_neighbor[1] < 0) ? 0 : each_rank_data + (rank_neighbor[1] < remain);
    }
  ```
  
* Odd-Even Sort
  每個for迴圈開始時，先取得鄰居資訊，並藉由 `MPI_Isend` 和 `MPI_Irecv` 傳遞資料給neighbors，進行**mergeData function**，以mergeFromMin(neighbor_rank < current_rank)判定tempBuffer vector存取資料對象，
  舉例來說
  ```
    for (int i = 0; i < size + 1; i++) {
        bool isOddPhase = i % 2;

        int neighbor;
        int neighbor_length;

        if (isOddPhase) {
            neighbor = rank_neighbor[1];
            neighbor_length = rank_neighbor_length[1];
        } else {
            neighbor = rank_neighbor[0];
            neighbor_length = rank_neighbor_length[0];
        }

        if (neighbor == MPI_PROC_NULL) {
            continue;
        }

        MPI_Request send_request, recv_request;

        MPI_Isend(data, each_rank_data_length, MPI_FLOAT, neighbor, 0, MPI_COMM_WORLD, &send_request);

        MPI_Irecv(recvData, neighbor_length, MPI_FLOAT, neighbor, 0, MPI_COMM_WORLD, &recv_request);

        MPI_Wait(&send_request, MPI_STATUS_IGNORE);
        MPI_Wait(&recv_request, MPI_STATUS_IGNORE);

        if (neighbor_length > 0) {
            bool mergeFromMin = (rank < neighbor);
            mergeData(data, each_rank_data_length, recvData, neighbor_length, tempBuffer, mergeFromMin);
            std::swap(data, tempBuffer);
        }
    }
  ```
  
  ```
  void mergeData(float* localData, int localSize, float* neighborData, int neighborSize, float* tempBuffer, bool mergeFromMin) {
      if (mergeFromMin) {
          int localData_index = 0, neighborData_index = 0;
  
          for(int tempBuffer_index = 0; tempBuffer_index < localSize ; tempBuffer_index++){
              if(localData_index < localSize && (neighborData_index >= neighborSize || localData[localData_index] < neighborData[neighborData_index])){
                  tempBuffer[tempBuffer_index] = localData[localData_index++];
              }else{
                  tempBuffer[tempBuffer_index] = neighborData[neighborData_index++];
              }
          }
      } else {
          int localData_index = localSize - 1;
          int neighborData_index = neighborSize - 1;
          
          for(int tempBuffer_index = localSize - 1; tempBuffer_index >= 0 ; tempBuffer_index--){
              if(localData_index >= 0 && (neighborData_index < 0 || localData[localData_index] > neighborData[neighborData_index])){
                  tempBuffer[tempBuffer_index] = localData[localData_index--];
              }else{
                  tempBuffer[tempBuffer_index] = neighborData[neighborData_index--];
              }
          }
      }
  }
  ```
* MPI Out
  ```

  ```


## Experiment & Analysis
### Methodology
#### System Spec
#### Performance Metrics

### Plots: Speedup Factor & Profile
#### Experimental Method:
#### Performance Measurement:
#### Analysis of Results:
#### Optimization Strategies:


## Experiences / Conclusion
