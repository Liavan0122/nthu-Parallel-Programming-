## Title, name, student ID
楊峻銘 112062576


## Implementation
### Pthread
* Load Balance
  
  由於qct-cpu，96 per node and 2 threaads each core，所以我直接把總工作量分成96個分支，並在執行程式前採用動態分配每個threads的工作量，
  將正方形總面積由上而下的按行劃分，讓每個threads運行不同行的元素。這是一個無限迴圈，每個線程都會嘗試獲取一行來計算。如果所有行都被分配，線程就會退出。
  ```
        int row;

        // Lock to get the current row
        pthread_mutex_lock(&mutex);
        if (current_row >= height) {
            pthread_mutex_unlock(&mutex);
            break; // Exit if all rows are processed
        }
        row = current_row;
        current_row++;
        pthread_mutex_unlock(&mutex);
  ```

  使用 AVX512 指令一次處理 8 個元素，因此每次 for 迴圈遞增 8。使用 _mm512_set_pd 指令將 8 個初始的 x0 值由座標左至右載入到 SIMD 暫存器 x0 中。
  y0為計算該行在複數平面上的值。
  ```
        double y0 = row * ((upper - lower) / height) + lower;
        for (int i = 0; i < width; i += 8) {
            // Load initial x0 values
            __m512d x0 = _mm512_set_pd(
                (i+7) * ((right - left) / width) + left,
                (i+6) * ((right - left) / width) + left,
                (i+5) * ((right - left) / width) + left,
                (i+4) * ((right - left) / width) + left,
                (i+3) * ((right - left) / width) + left,
                (i+2) * ((right - left) / width) + left,
                (i+1) * ((right - left) / width) + left,
                i * ((right - left) / width) + left
            );
  ```

* SIMD+Algo
  
`Intel R AVX-512 family` 指令集可以加速計算，演算法概念部分和sequencial雷同，新增了`repeats`變量紀錄每個元素迭代次數，將 SIMD 暫存器中的 repeats 結果存儲回緩衝區 buffer 中， buffer為整個面積元素對應位置。
```
            // Mandelbrot iteration
            for (int k = 0; k < iters; ++k) {
                __m512d x2 = _mm512_mul_pd(x, x);
                __m512d y2 = _mm512_mul_pd(y, y);
                length_squared = _mm512_add_pd(x2, y2);

                // Check if length_squared >= 4
                __mmask8 mask = _mm512_cmp_pd_mask(length_squared, four, _CMP_LT_OQ);
                active_mask &= mask;

                if (active_mask == 0) {
                    break; // Exit if all pixels have escaped
                }

                // Mandelbrot iteration formula
                __m512d xy = _mm512_mul_pd(x, y);
                __m512d temp = _mm512_sub_pd(x2, y2);
                x = _mm512_add_pd(temp, x0);
                y = _mm512_add_pd(_mm512_mul_pd(xy, two), y0_vec);

                // Update repeats using mask
                repeats = _mm512_mask_add_epi32(repeats, mask, repeats, _mm512_set1_epi32(1));
            }

            // Store results back to the buffer
            int buffer_offset = row * width + i;
            int repeat_vals[8];
            _mm512_storeu_si512((__m512i*)repeat_vals, repeats);
            for (int k = 0; k < 8 && (i + k) < width; ++k) {
                buffer[buffer_offset + k] = repeat_vals[k];
            }
```

### Hybrid

* Load Balance
  
  工作量平均分給每個節點，在每個rank底下，分配給多個threads，並且 OpenMP 並行部分的工作分配方式使用 `schedule(dynamic)`，而且原來的 pthread 使用了互斥鎖來控制 current_row 的更新，但在 OpenMP 中使用更為高效的 schedule 子句來自動分配行給線程。
  
  ```
    long long unit = height / size;
    long long remain = height % size;
    long long start_row = unit * rank + (rank < remain ? rank : remain);
    long long num_rows = unit + (rank < remain ? 1 : 0);
    image = (int*)malloc(num_rows * width * sizeof(int));

  ```
  ```
    #pragma omp for schedule(dynamic) reduction(+:current_row)
  ```
  
* SIMD+Algo
  
  整體概念與pthread相同，這裡改嘗試用reduction最終合併。
  
* Gather
  收集所有 MPI 節點計算的結果，最終由 Rank 0 節點來匯總和生成最終的 PNG 圖像。
  ```
    if (rank == 0) {
        int* full_image = (int*)malloc(total * sizeof(int));
        assert(full_image);
        memcpy(full_image + start_row * width, image, num_rows * width * sizeof(int));

        for (int i = 1; i < size; i++) {
            long long recv_start_row = unit * i + (i < remain ? i : remain);
            long long recv_num_rows = unit + (i < remain ? 1 : 0);
            MPI_Recv(full_image + recv_start_row * width, recv_num_rows * width, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        write_png(filename, iters, width, height, full_image);
        free(full_image);
    } else {
        MPI_Send(image, num_rows * width, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
  ```


## Experiment & Analysis
### Plots: Scalability & Load Balancing & Profile
#### System Spec  

課程所提供的qct-cpu

#### Experimental Method:

以*testcase strict34.txt* 作為測資，因為他花時間很長  

**strict34.txt** : 10000 -0.5506164691618783 -0.5506164628264113 0.6273445437118131 0.6273445403522527 7680 4320

#### Performance Measurement:
使用 Intel Vtune 觀測


#### Analysis of Results:

![image](https://github.com/user-attachments/assets/4731db76-5c78-40b5-b4ba-226db0a5c515)
這是pthread，因為我開了96個threads，所以可以看到它可以展開超長，但每個cpu utilization普遍偏低，其實也不是開越多threads就會越快，還要考慮到context switch的時間

![image](https://github.com/user-attachments/assets/e31587d2-2db0-4b66-9cdc-fec6186bbce2)
可以看到幾乎所有執行時間都花在**mandelbrot_thread**上，很正常，因為這裡才是最大量計算的地方，所以會看到下半部的`CPU Utilization Histogram`很差(poor)，畢竟只有一個主要平行的function。
VTune 中顯示 mandelbrot_thread 的 CPU 使用時間是 239.4 秒，這代表所有線程在 CPU 上工作的累積時間。
這是我的猜測，因為我有 **96** 個線程，並且它們每個線程都獲得大約 **2.4** 秒 的 CPU 使用時間，那麼累加起來就是接近 **240** 秒 的總 CPU 時間。  


![image](https://github.com/user-attachments/assets/18496d62-4ff8-4c98-9218-455e25f7db6f)
圖中最右側點開後，為所花時間極少的`PNG_write`或是`main`部分

---------------------

![image](https://github.com/user-attachments/assets/e7d4b782-dd2b-4db9-acc2-04f8948c5bd4)
這是hybrid，可以看到hw2b(TID:787735)相較(TID:787755) `CPU Time`更為長一點，因為後續**gather + write PNG**動作。

![image](https://github.com/user-attachments/assets/d4fb8cab-c52c-4e3d-b419-2fdb2bf257dc)

hybrid相較pthread多樣，通常需要在不同的模型之間進行同步和協作。例如，在 MPI 和 OpenMP 混合的情境下，你可能需要在各個節點間傳遞數據（MPI 通訊），同時在節點內部進行多線程同步，
在flame graph 中，可以看到向上凸的就是反映出各種額外的同步開銷

![image](https://github.com/user-attachments/assets/b8b93c96-5a36-4031-bb06-9d7b93a5a27d)

像 mca_pml_ucx_recv、uct_mm_iface_progress、ucp_worker_progress 這些函數和符號是與 MPI 和 UCX 通訊層相關的，它們通常出現在 Hybrid 中，
特別是在使用 MPI 進行跨節點分佈式計算時會出現。而這些是 Pthreads 編程環境中不會出現的，因為 Pthreads 只處理單節點內的多線程並行，並不涉及跨節點的通訊和數據傳輸。

### Discussion
ALL single node on qct-cpu

#### Pthread Scalability
在pthread中，我是採用`<sys/time>`的`gettimeofday()`函數計算時間，主要計算threads分散出去的`mandelbrot_thread`部分，不包含`write_png`時間。
![image](https://github.com/user-attachments/assets/61d9da12-020f-44c5-8031-3ebfcd119124)
從圖中可以看出，隨著線程數量的增加，執行時間顯著減少，但只要超過48個數量，是有小幅動增加時間的，整個程式主要花費計算的地方並沒有過多的溝通，也沒有IO影響，彼此獨立。並在Load balance也是動態調整每個thread的工作量，
在*Analysis of Results*的部分也可看出相當平均，所以就不呈現圖了。

#### Hybrid
在Hybrid，我是採用`MPI_Wtime()`函數計算時間，分別記錄不同的process數量對應的執行時間曲線，且分別記錄loading balance所花時間 和 Mandelbrot function 和 write_png 部分。

| ![image](https://github.com/user-attachments/assets/8e76e5ea-c276-4ff8-977f-4616d67dd514) | ![image](https://github.com/user-attachments/assets/736eaf51-675b-4b50-a6db-ab6f71d1b82c) | ![image](https://github.com/user-attachments/assets/50a356f2-d6bc-4c31-bc70-924e27660482) |
|:-------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------:|
| Time Taken to Calculate the Range                  | Time Taken for Mandelbrot Set Calculation                      | Time Taken to Gather Results to Rank 0            |


這三張圖表分別顯示了每個主要步驟在不同 -np 值（進程數量）下的時間，分別為16 24 48 96 192，可以觀察到 Mandelbrot 在數量96之前有線性加快， 可是當 -np 增加到 96 及以上時各process commucations通訊量遽增，導致收集時間增加，在48~96之間會是一個比較平衡的狀態。


## Conclusion

在這次作業中，真的學到很多很多的新知識...，彷彿學習是沒有盡頭的一樣，從平行套件pthread or openmpi or hybrid，每個套件自己也有不同的互斥、排程、交互作用，再來也要研究指令集的運用方法，AVX和SSE這些東西，如果對他們沒有一定基礎的了解，會不知道在特定場合是否可以這樣用到此暫存器引用的方式，最後程式寫出來後也要好好的研究profilers的使用，是很多的酷東西啦，尤其是SIMD的部分，是沒想過可以碰到這麼底層的存取。回到程式碼部分，我比較隨興，能平均就平均，能dynamic就自動動態，壞處就是不會是最少時效，但意外地只要能好好的分配，整體的效能就容易有大幅度提升，測試過指令集AVX512改成AVX128也是一定大跨越，看到了其強大之處。但也在學習新的指令集研究最久時間，而且看了又忘，忘了又看。但藉由這次作業，很好的學習到這兩種平行方式在通訊上面的不同還有在不同的scalibility中有趣的狀況。總之，從這個作業獲益匪淺。
